> Loaded logger: ./results/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- LOAD DATA -> (['train', 'eval']) ---]
> Load/Init from ./data/tweets.train.csv
> Load/Init from ./data/tweets.eval.csv

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 11.0146 sec
> Init BERT-Head (MLP), trainable parameters: 197635

[--- TRAIN -> ./data/tweets.train.csv ---]
@001: 	loss(train)=0.6937 	loss(eval)=0.5672 	f1(train)=0.7137 	f1(eval)=0.7698 	duration(epoch)=0:00:30.519466
@002: 	loss(train)=0.5636 	loss(eval)=0.5453 	f1(train)=0.7729 	f1(eval)=0.7801 	duration(epoch)=0:00:30.014494
@003: 	loss(train)=0.5306 	loss(eval)=0.5193 	f1(train)=0.7907 	f1(eval)=0.7913 	duration(epoch)=0:00:30.072000
@004: 	loss(train)=0.5121 	loss(eval)=0.5027 	f1(train)=0.7956 	f1(eval)=0.7971 	duration(epoch)=0:00:30.227731
@005: 	loss(train)=0.4930 	loss(eval)=0.5065 	f1(train)=0.8048 	f1(eval)=0.7930 	duration(epoch)=0:00:30.172716
@006: 	loss(train)=0.4920 	loss(eval)=0.5194 	f1(train)=0.8050 	f1(eval)=0.7872 	duration(epoch)=0:00:30.146026
@007: 	loss(train)=0.4882 	loss(eval)=0.4965 	f1(train)=0.8086 	f1(eval)=0.8019 	duration(epoch)=0:00:30.218367
@008: 	loss(train)=0.4789 	loss(eval)=0.4953 	f1(train)=0.8082 	f1(eval)=0.7982 	duration(epoch)=0:00:30.162432
@009: 	loss(train)=0.4695 	loss(eval)=0.5117 	f1(train)=0.8130 	f1(eval)=0.7920 	duration(epoch)=0:00:30.004075
@010: 	loss(train)=0.4599 	loss(eval)=0.4933 	f1(train)=0.8188 	f1(eval)=0.8057 	duration(epoch)=0:00:30.209758
@011: 	loss(train)=0.4560 	loss(eval)=0.4977 	f1(train)=0.8204 	f1(eval)=0.8002 	duration(epoch)=0:00:29.858025
@012: 	loss(train)=0.4519 	loss(eval)=0.5063 	f1(train)=0.8196 	f1(eval)=0.7968 	duration(epoch)=0:00:30.345957
@013: 	loss(train)=0.4422 	loss(eval)=0.4962 	f1(train)=0.8264 	f1(eval)=0.8005 	duration(epoch)=0:00:30.003894
@014: 	loss(train)=0.4484 	loss(eval)=0.5294 	f1(train)=0.8256 	f1(eval)=0.7910 	duration(epoch)=0:00:29.990922
@015: 	loss(train)=0.4370 	loss(eval)=0.4990 	f1(train)=0.8277 	f1(eval)=0.8002 	duration(epoch)=0:00:30.093387
@016: 	loss(train)=0.4364 	loss(eval)=0.5094 	f1(train)=0.8282 	f1(eval)=0.7978 	duration(epoch)=0:00:29.748615
@017: 	loss(train)=0.4372 	loss(eval)=0.5335 	f1(train)=0.8241 	f1(eval)=0.7893 	duration(epoch)=0:00:30.202736
@018: 	loss(train)=0.4237 	loss(eval)=0.5011 	f1(train)=0.8314 	f1(eval)=0.7999 	duration(epoch)=0:00:30.252520
@019: 	loss(train)=0.4225 	loss(eval)=0.5466 	f1(train)=0.8342 	f1(eval)=0.7927 	duration(epoch)=0:00:30.227088
@020: 	loss(train)=0.4258 	loss(eval)=0.5022 	f1(train)=0.8332 	f1(eval)=0.8026 	duration(epoch)=0:00:29.646808
@021: 	loss(train)=0.4099 	loss(eval)=0.5248 	f1(train)=0.8375 	f1(eval)=0.7975 	duration(epoch)=0:00:29.921825
@022: 	loss(train)=0.4176 	loss(eval)=0.5070 	f1(train)=0.8337 	f1(eval)=0.8016 	duration(epoch)=0:00:30.050709
@023: 	loss(train)=0.4184 	loss(eval)=0.5057 	f1(train)=0.8351 	f1(eval)=0.7975 	duration(epoch)=0:00:29.899771
@024: 	loss(train)=0.4018 	loss(eval)=0.5089 	f1(train)=0.8407 	f1(eval)=0.8057 	duration(epoch)=0:00:29.792332
@025: 	loss(train)=0.4019 	loss(eval)=0.5105 	f1(train)=0.8397 	f1(eval)=0.7982 	duration(epoch)=0:00:29.787789
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197635
@024: 	loss(train)=0.4018 	loss(eval)=0.5089 	f1(train)=0.8407 	f1(eval)=0.8057 	duration(epoch)=0:00:29.792332

[--- EVAL -> ./data/tweets.eval.csv ---]
AVG           	 tp:     2359	 fp:      569 	 tn:     4718	 fn:      569	 pre=0.8057	 rec=0.8057	 f1=0.8057	 acc=0.8615
neutral       	 tp:      369	 fp:      178 	 tn:     1990	 fn:      264	 pre=0.6746	 rec=0.5829	 f1=0.6254	 acc=0.8422
negative      	 tp:     1663	 fp:      306 	 tn:      696	 fn:      158	 pre=0.8446	 rec=0.9132	 f1=0.8776	 acc=0.8356
positive      	 tp:      327	 fp:       85 	 tn:     2032	 fn:      147	 pre=0.7937	 rec=0.6899	 f1=0.7381	 acc=0.9105
